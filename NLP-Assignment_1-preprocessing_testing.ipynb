{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Preprocessing Testing Dataset"]},{"cell_type":"markdown","metadata":{},"source":["This notebook constitutes the second pipeline of the N-Gram Language Modeling Analysis Project. It includes the preprocessing of the testing data from the DUC 2005 dataset."]},{"cell_type":"markdown","metadata":{},"source":["After the execution of this notebook, the following output files will be generated and saved in the folder /output_data:\n","- test_sentences.pkl: List of sentences in the test set after preprocessing, before tagging \\<UNK\\> words.\n","- test_sentences_unk.pkl: List of sentences in the test set after preprocessing, after tagging \\<UNK\\> words.\n","- unigram_dictionary_test.pkl: Dictionary containing all the unigrams of the test set after preprocessing.\n","- bigram_dictionary_test.pkl: Dictionary containing all the biigrams of the test set after preprocessing.\n","- trigram_dictionary_test.pkl: Dictionary containing all the triigrams of the test set after preprocessing.\n","- fourgram_dictionary_test.pkl: Dictionary containing all the fourgrams of the test set after preprocessing."]},{"cell_type":"markdown","metadata":{},"source":["The output files of this notebook will be used in the other pipelines: \n","- NLP-Assignment_2-analytics.ipynb\n","- NLP-Assignment_3-results.ipynb"]},{"cell_type":"markdown","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import nltk\n","from bs4 import BeautifulSoup\n","import os\n","import re\n","import itertools\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"o1eMFuEGcqd5"},"source":["## Read HTML Documents"]},{"cell_type":"markdown","metadata":{"id":"whzQkbsYcqd6"},"source":["### Get path and name of all the files containd in the Test dataset"]},{"cell_type":"code","execution_count":107,"metadata":{"id":"OZuqnAkTcqd6"},"outputs":[],"source":["import os\n","\n","#Get the file names (either assessments or measurments, does not matter since we are only counting rows)\n","mypath = \"D:/Datos/Documents/Development/Python/Projects/NCF/NLP/Assignment01/DUC 2005 Dataset/TestSet\"\n","#mypath = \"/Users/milevavantuyl/Desktop/NLP/Assignment 1/google_drive/DUC 2005 Dataset\" # Mileva's path\n","all_files = []\n","\n","for path, subdirs, files in os.walk(mypath):\n","    for name in files:\n","        all_files.append(os.path.join(path, name))"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"Sr-Jr2tBcqd6"},"outputs":[],"source":["all_files = [file for file in all_files if '.DS_Store' not in file] # Mileva remove . files"]},{"cell_type":"markdown","metadata":{"id":"hRb1gU5acqd6"},"source":["### Join all corpus in a single variable"]},{"cell_type":"code","execution_count":109,"metadata":{"id":"R4OjNSqycqd6","outputId":"de31ec62-b610-49da-a1db-80e8e728abf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 270 files in the training corpus.\n"]}],"source":["#html_document = \"D:/Datos/Documents/Development/Python/Projects/NCF/NLP/Assignment01/DUC 2005 Dataset/TrainingSet/d301i/FT921-10162\"\n","\n","corpus = []\n","\n","for file in all_files:\n","    with open(file, 'r') as f:\n","        contents = f.read()\n","        soup = BeautifulSoup(contents, 'html.parser')\n","        corpus.append(soup.text)\n","\n","num_training_documents = len(corpus)\n","print(f\"There are {num_training_documents} files in the training corpus.\")"]},{"cell_type":"code","execution_count":110,"metadata":{"id":"sbGZI55vcqd7"},"outputs":[],"source":["#Corpus contains a list of corpus. Join all elements in a single corpus\n","text = ''.join(corpus)\n","#text"]},{"cell_type":"markdown","metadata":{"id":"4zvACeDCcqd8"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"FMsJdtadcqd8"},"source":["### Lowercase"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"aI1HCWSycqd8"},"outputs":[],"source":["text = text.lower()\n","#text"]},{"cell_type":"markdown","metadata":{"id":"UtvUsoN8cqd8"},"source":["## Handle trailing spaces"]},{"cell_type":"code","execution_count":112,"metadata":{"id":"WVObjaVhcqd8"},"outputs":[],"source":["#Substitute \\n with blank space\n","text = text.replace('\\n', ' ').replace('\\r', '')\n","#text"]},{"cell_type":"markdown","metadata":{"id":"kODQFzEZcqd8"},"source":["### Create List of Sentences"]},{"cell_type":"markdown","metadata":{"id":"cUvkIoyScqd9"},"source":["Mileva updated the order. Created the list of sentences & such before identifying the unknown words. "]},{"cell_type":"code","execution_count":113,"metadata":{"id":"QsxXo8Ykcqd9","outputId":"ae4478ac-dffb-4978-a943-c660df7c58ad"},"outputs":[{"data":{"text/plain":["\"   la011089-0104   3661    january 10, 1989, tuesday, home edition      metro; part 2; page 3; column 1; metro desk      570 words      saluting the heroes;    deputies honored for risking their lives trying to save others      by william overend, times staff writer      sheriff's deputies tim parker and gean okada risked their lives trying to save  two small children from a burning house in south-central los angeles.\""]},"execution_count":113,"metadata":{},"output_type":"execute_result"}],"source":["import nltk.data\n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","sentences = tokenizer.tokenize(text)\n","len(sentences)\n","sentences[0]"]},{"cell_type":"markdown","metadata":{"id":"kPMAyirscqd9"},"source":["### Remove Special Characters"]},{"cell_type":"code","execution_count":114,"metadata":{"id":"D5Kzorw3cqd9","outputId":"e773be41-ba3b-407c-cf56-34df3a27ef60"},"outputs":[{"data":{"text/plain":["'   la0110890104   3661    january 10 1989 tuesday home edition      metro part 2 page 3 column 1 metro desk      570 words      saluting the heroes    deputies honored for risking their lives trying to save others      by william overend times staff writer      sheriffs deputies tim parker and gean okada risked their lives trying to save  two small children from a burning house in southcentral los angeles'"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["clean_sentences = []\n","\n","for sentence in sentences:\n","    clean = re.sub(r\"[^a-zA-Z0-9?! ]+\", \"\", sentence)\n","    clean_sentences.append(clean)\n","\n","clean_sentences[0]"]},{"cell_type":"markdown","metadata":{"id":"xjdpZ8Azcqd9"},"source":["### Tokenice Sentences"]},{"cell_type":"code","execution_count":115,"metadata":{"id":"0MSc0gdzcqd9"},"outputs":[],"source":["# A list of sentences, where each sentence is represented as a list of tokens\n","tokenized_sentences = []\n","for sentence in clean_sentences:\n","    tokens = nltk.word_tokenize(sentence)\n","    tokenized_sentences.append(tokens)"]},{"cell_type":"markdown","metadata":{},"source":["### Remove long sentences"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"data":{"text/plain":["(9152, 8886)"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["all_sentences = tokenized_sentences\n","tokenized_sentences = [] #Will store the selected sentences (#tokens<55)\n","[tokenized_sentences.append(sentence) for sentence in all_sentences if len(sentence) < 55]\n","\n","len(all_sentences), len(tokenized_sentences)"]},{"cell_type":"markdown","metadata":{"id":"h4K_zLgqcqd9"},"source":["### Save Cleaned/ Tokenized Sentences (Mileva addition)"]},{"cell_type":"code","execution_count":118,"metadata":{"id":"1Ip1n7xlcqd-"},"outputs":[],"source":["# Save tokenized sentences (before adding the <UNK> tag)\n","filename = os.path.join(\"output_data\", \"UNK 5-55/test_sentences.pkl\")\n","with open(filename, \"wb\") as file: \n","    pickle.dump(tokenized_sentences, file)"]},{"cell_type":"markdown","metadata":{"id":"RE2SL83wcqd-"},"source":["### Tag Unknown Words"]},{"cell_type":"markdown","metadata":{"id":"JNhAVDu3cqd-"},"source":["We are tagging with \\<UNK\\> all the words that are not contained in the used_words list, generated with the training set."]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"data":{"text/plain":["['ft',\n"," '07',\n"," 'feb',\n"," '92',\n"," 'noriega',\n"," 'gains',\n"," 'ground',\n"," 'henry',\n"," 'hamman',\n"," 'focuses',\n"," 'on',\n"," 'the',\n"," 'former',\n"," 'panamanian',\n"," 'leaders',\n"," 'attempts',\n"," 'to',\n"," 'fend',\n"," 'off',\n"," 'us',\n"," 'drugs',\n"," 'charges',\n"," 'by',\n"," 'leader',\n"," 'general',\n"," 'manuel',\n"," 'antonio',\n"," 'noriegas',\n"," 'defence',\n"," 'against',\n"," 'drug',\n"," 'trafficking',\n"," 'in',\n"," 'miami',\n"," 'gained',\n"," 'this',\n"," 'week',\n"," 'a',\n"," 'enforcement',\n"," 'administration',\n"," 'agent',\n"," 'acknowledged',\n"," 'that',\n"," 'man',\n"," 'identified',\n"," 'prosecution',\n"," 'as',\n"," 'medelln',\n"," 'cocaine',\n"," 'cartel',\n"," 'money',\n"," 'launderer',\n"," 'had',\n"," 'been',\n"," 'arrested',\n"," 'basis',\n"," 'of',\n"," 'information',\n"," 'from',\n"," 'forces',\n"," 'commanded',\n"," 'gen',\n"," 'testimony',\n"," 'mr',\n"," 'james',\n"," 'l',\n"," 'bramble',\n"," 'charge',\n"," 'panama',\n"," 'operations',\n"," 'august',\n"," '1982',\n"," 'until',\n"," 'june',\n"," '1984',\n"," 'raised',\n"," 'questions',\n"," 'about',\n"," 'earlier',\n"," 'claims',\n"," 'protected',\n"," 'cartels',\n"," 'return',\n"," 'for',\n"," 'payoffs',\n"," 'prosecutors',\n"," 'cited',\n"," 'ramon',\n"," 'milian',\n"," 'rodriguez',\n"," 'conduit',\n"," 'bank',\n"," 'accounts',\n"," 'controlled',\n"," 'according',\n"," 'dea',\n"," 'was',\n"," 'responsible',\n"," 'transferring',\n"," 'dollars',\n"," '200m',\n"," 'profits',\n"," 'led',\n"," 'arrest',\n"," 'independently',\n"," 'developed',\n"," 'narcotics',\n"," 'officers',\n"," 'and',\n"," 'passed',\n"," 'when',\n"," 'florida',\n"," 'government',\n"," 'agents',\n"," 'seized',\n"," '55m',\n"," 'cash',\n"," 'more',\n"," 'than',\n"," '61',\n"," 'pounds',\n"," 'lear',\n"," 'jet',\n"," 'aircraft',\n"," 'calling',\n"," 'current',\n"," 'officials',\n"," 'requiring',\n"," 'them',\n"," 'read',\n"," 'documents',\n"," 'they',\n"," 'wrote',\n"," 'efforts',\n"," 'law',\n"," 'is',\n"," 'attempting',\n"," 'counter',\n"," 'claim',\n"," 'turned',\n"," 'into',\n"," 'haven',\n"," 'colombian',\n"," 'dealers',\n"," 'another',\n"," 'chief',\n"," 'administrator',\n"," 'john',\n"," 'lawn',\n"," 'argued',\n"," 'kind',\n"," 'words',\n"," 'may',\n"," 'have',\n"," 'written',\n"," 'were',\n"," 'diplomatic',\n"," 'niceties',\n"," 'rather',\n"," 'statements',\n"," 'fact',\n"," 'but',\n"," 'letters',\n"," 'also',\n"," 'referred',\n"," 'cooperation',\n"," 'between',\n"," 'under',\n"," 'command',\n"," 'frank',\n"," 'rubino',\n"," 'lead',\n"," 'counsel',\n"," 'said',\n"," 'an',\n"," 'interview',\n"," 'showing',\n"," 'extent',\n"," 'central',\n"," 'part',\n"," 'strategy',\n"," 'has',\n"," 'testified',\n"," 'he',\n"," 'expected',\n"," 'be',\n"," 'followed',\n"," 'who',\n"," 'worked',\n"," 'expect',\n"," 'finish',\n"," 'presenting',\n"," 'their',\n"," 'case',\n"," '46',\n"," 'weeks',\n"," 'offer',\n"," 'financial',\n"," 'times',\n"," 'london',\n"," 'page',\n"," '4',\n"," '10',\n"," 'apr',\n"," 'guilty',\n"," 'main',\n"," 'counts',\n"," 'trial',\n"," 'george',\n"," 'graham',\n"," 'washington',\n"," 'federal',\n"," 'jury',\n"," 'yesterday',\n"," 'found',\n"," 'eight',\n"," 'ten',\n"," 'conspiracy',\n"," 'verdict',\n"," 'after',\n"," 'nearly',\n"," 'five',\n"," 'days',\n"," 'deliberation',\n"," 'means',\n"," 'could',\n"," 'face',\n"," 'up',\n"," '120',\n"," 'years',\n"," 'jails',\n"," 'first',\n"," 'foreign',\n"," 'head',\n"," 'brought',\n"," 'chains',\n"," 'january',\n"," '1990',\n"," 'invasion',\n"," 'previous',\n"," 'month',\n"," 'particularly',\n"," 'bush',\n"," 'decision',\n"," 'try',\n"," 'common',\n"," 'criminal',\n"," 'risky',\n"," 'move',\n"," 'will',\n"," 'widely',\n"," 'seen',\n"," 'bushs',\n"," 'send',\n"," 'topple',\n"," 'stood',\n"," 'entered',\n"," 'with',\n"," 'based',\n"," 'colombia',\n"," 'use',\n"," 'route',\n"," 'shipment',\n"," 'thousands',\n"," 'tons',\n"," 'laundering',\n"," 'portrayed',\n"," 'having',\n"," 'sold',\n"," 'out',\n"," 'his',\n"," 'country',\n"," 'millions',\n"," 'bribes',\n"," 'agreed',\n"," 'prosecutions',\n"," 'used',\n"," 'president',\n"," 'fidel',\n"," 'castro',\n"," 'cuba',\n"," 'mediate',\n"," 'dispute',\n"," 'him',\n"," 'itself',\n"," 'likely',\n"," 'supporters',\n"," 'strengthened',\n"," 'trade',\n"," 'embargo',\n"," 'issue',\n"," 'being',\n"," 'debated',\n"," 'congress',\n"," 'two',\n"," 'racketeering',\n"," 'conviction',\n"," 'these',\n"," 'alone',\n"," '40',\n"," 'jail',\n"," '500000',\n"," 'fines',\n"," 'sentencing',\n"," 'due',\n"," 'july',\n"," 'appeal',\n"," 'came',\n"," 'day',\n"," 'pronounced',\n"," 'deadlocked',\n"," 'ordered',\n"," 'deliberations',\n"," 'judge',\n"," 'only',\n"," 'which',\n"," 'not',\n"," 'involved',\n"," 'smuggling',\n"," 'several',\n"," 'hundred',\n"," 'kilogrammes',\n"," 'pleasure',\n"," 'boat',\n"," 'convictions',\n"," 'relieve',\n"," 'possibility',\n"," 'much',\n"," 'embarrassment',\n"," 'arising',\n"," 'acquittal',\n"," 'might',\n"," 'faced',\n"," 'why',\n"," 'it',\n"," 'went',\n"," 'so',\n"," 'trouble',\n"," 'no',\n"," 'avail',\n"," 'especially',\n"," 'establishment',\n"," 'democratic',\n"," 'antidrug',\n"," 'now',\n"," 'appears',\n"," 'doubt',\n"," 'deployed',\n"," 'vast',\n"," 'sums',\n"," 'bring',\n"," 'book',\n"," 'spent',\n"," '1989',\n"," 'some',\n"," '20m',\n"," 'there',\n"," 'little',\n"," 'likelihood',\n"," 'encouraged',\n"," 'its',\n"," 'success',\n"," 'same',\n"," 'approach',\n"," 'again',\n"," 'near',\n"," 'future',\n"," 'pursuing',\n"," 'colonel',\n"," 'muammer',\n"," 'gadaffi',\n"," 'libya',\n"," 'example',\n"," 'preferred',\n"," 'work',\n"," 'through',\n"," 'un',\n"," '5',\n"," 'jul',\n"," 'survey',\n"," 'italy',\n"," '15',\n"," 'message',\n"," 'mafia',\n"," 'robert',\n"," 'official',\n"," 'organised',\n"," 'crime',\n"," 'impact',\n"," 'assassination',\n"," 'giovanni',\n"," 'falcone',\n"," 'italys',\n"," 'leading',\n"," 'antimafia',\n"," 'magistrate',\n"," 'take',\n"," 'long',\n"," 'time',\n"," 'erase',\n"," 'killed',\n"," 'along',\n"," 'wife',\n"," 'three',\n"," 'bodyguards',\n"," '24',\n"," 'ton',\n"," 'high',\n"," 'explosive',\n"," 'detonated',\n"," 'convoy',\n"," 'travelled',\n"," 'at',\n"," 'speed',\n"," 'motorway',\n"," 'palermo',\n"," 'airport',\n"," 'employed',\n"," 'reminiscent',\n"," 'lebanese',\n"," 'militias',\n"," 'or',\n"," 'barons',\n"," 'horrendous',\n"," 'unequivocally',\n"," 'attributed',\n"," 'sicilian',\n"," 'target',\n"," 'sadly',\n"," 'campaigning',\n"," 'falcones',\n"," 'investigations',\n"," 'series',\n"," '1980s',\n"," 'made',\n"," 'extensive',\n"," 'pentiti',\n"," 'informers',\n"," 'equivalent',\n"," 'supergrass',\n"," 'able',\n"," 'demonstrate',\n"," 'casual',\n"," 'group',\n"," 'criminals',\n"," 'complex',\n"," 'organisation',\n"," 'hierarchy',\n"," 'linking',\n"," 'various',\n"," 'families',\n"," 'businesses',\n"," 'marked',\n"," 'already',\n"," 'subject',\n"," 'failed',\n"," 'attempt',\n"," 'one',\n"," 'most',\n"," 'heavily',\n"," 'guarded',\n"," 'people',\n"," 'least',\n"," '60',\n"," 'assigned',\n"," 'fulltime',\n"," 'protection',\n"," 'movements',\n"," 'kept',\n"," 'deeply',\n"," 'confidential',\n"," 'killing',\n"," 'deliberate',\n"," 'challenge',\n"," 'state',\n"," 'similar',\n"," 'carlo',\n"," 'alberto',\n"," 'dalla',\n"," 'chiesa',\n"," 'taken',\n"," 'post',\n"," 'just',\n"," 'prodded',\n"," 'react',\n"," 'police',\n"," 'judicial',\n"," 'measures',\n"," 'aid',\n"," 'fight',\n"," 'did',\n"," 'murder',\n"," 'procedures',\n"," 'bail',\n"," 'suspected',\n"," 'members',\n"," 'tightened',\n"," 'magistrates',\n"," 'given',\n"," 'greater',\n"," 'freedom',\n"," 'pursue',\n"," 'new',\n"," 'mechanisms',\n"," 'introduced',\n"," 'better',\n"," 'coordination',\n"," 'reactive',\n"," 'characterised',\n"," 'action',\n"," 'towards',\n"," 'ever',\n"," 'since',\n"," 'second',\n"," 'world',\n"," 'war',\n"," 'instance',\n"," 'last',\n"," 'september',\n"," 'businessman',\n"," 'murdered',\n"," 'publicly',\n"," 'refusing',\n"," 'pay',\n"," 'extortion',\n"," 'demands',\n"," 'authorities',\n"," 'amid',\n"," 'introduce',\n"," 'included',\n"," 'special',\n"," 'denounce',\n"," 'assistance',\n"," 'those',\n"," 'suffering',\n"," 'losses',\n"," 'result',\n"," 'insurance',\n"," 'coverage',\n"," 'fire',\n"," 'damage',\n"," 'etc',\n"," 'became',\n"," 'operative',\n"," 'almost',\n"," 'nine',\n"," 'months',\n"," 'approved',\n"," 'cabinet',\n"," 'tends',\n"," 'tackle',\n"," 'effects',\n"," 'causes',\n"," 'too',\n"," 'often',\n"," 'are',\n"," 'simply',\n"," 'added',\n"," 'existing',\n"," 'ones',\n"," 'creating',\n"," 'confusion',\n"," 'year',\n"," 'response',\n"," 'spread',\n"," 'andreotti',\n"," 'decided',\n"," 'create',\n"," 'structure',\n"," 'within',\n"," 'deal',\n"," 'exclusively',\n"," 'consists',\n"," '26',\n"," 'assistants',\n"," 'however',\n"," 'lines',\n"," 'authority',\n"," 'still',\n"," 'remain',\n"," 'unclear',\n"," 'wary',\n"," 'body',\n"," 'because',\n"," 'smacks',\n"," 'direct',\n"," 'control',\n"," 'wrangle',\n"," 'delayed',\n"," 'introduction',\n"," 'indeed',\n"," 'before',\n"," 'death',\n"," 'blocked',\n"," 'fear',\n"," 'appointment',\n"," 'political',\n"," 'previously',\n"," 'coordinated',\n"," 'such',\n"," 'wound',\n"," 'traditionally',\n"," 'southern',\n"," 'phenomenon',\n"," 'associated',\n"," 'groups',\n"," 'each',\n"," 'distinguished',\n"," 'clear',\n"," 'geographical',\n"," 'historical',\n"," 'characteristics',\n"," 'cosa',\n"," 'nostra',\n"," 'camorra',\n"," 'round',\n"," 'naples',\n"," 'ndrangheta',\n"," 'though',\n"," 'retaining',\n"," 'strong',\n"," 'roots',\n"," 'rome',\n"," 'cities',\n"," 'northern',\n"," 'biggest',\n"," 'international',\n"," 'connections',\n"," 'dealing',\n"," 'arms',\n"," 'contraband',\n"," 'parts',\n"," 'ties',\n"," 'well',\n"," 'moving',\n"," 'eastern',\n"," 'europe',\n"," 'remained',\n"," 'local',\n"," 'continues',\n"," 'specialise',\n"," 'latin',\n"," 'america',\n"," 'heroin',\n"," 'turkey',\n"," 'asia',\n"," 'southeast',\n"," 'advances',\n"," 'telecommunications',\n"," 'liberalisation',\n"," 'banking',\n"," 'freer',\n"," 'opening',\n"," 'provided',\n"," 'enormous',\n"," 'stimulus',\n"," 'growth',\n"," 'italian',\n"," 'recently',\n"," 'civil',\n"," 'conflict',\n"," 'balkans',\n"," 'created',\n"," 'fresh',\n"," 'opportunities',\n"," 'while',\n"," 'activity',\n"," 'increasing',\n"," 'above',\n"," 'european',\n"," 'average',\n"," 'third',\n"," 'all',\n"," 'homicides',\n"," '1985',\n"," 'number',\n"," 'murders',\n"," 'doubled',\n"," 'britain',\n"," 'germany',\n"," 'rate',\n"," 'declined',\n"," 'become',\n"," 'entrenched',\n"," 'going',\n"," 'increasingly',\n"," 'hard',\n"," 'eradicate',\n"," 'becoming',\n"," 'difficult',\n"," 'distinguish',\n"," 'licit',\n"," 'illicit',\n"," 'funds',\n"," 'report',\n"," 'released',\n"," 'censis',\n"," 'social',\n"," 'research',\n"," 'institute',\n"," 'annual',\n"," 'turnover',\n"," 'crimes',\n"," 'activities',\n"," 'other',\n"," 'put',\n"," 'figure',\n"," 'over',\n"," 'estimated',\n"," '19',\n"," 'per',\n"," 'cent',\n"," 'earnings',\n"," 'public',\n"," 'contracts',\n"," 'sicily',\n"," '75',\n"," 'do',\n"," 'even',\n"," 'go',\n"," 'tender',\n"," 'virtually',\n"," 'latter',\n"," 'considered',\n"," 'source',\n"," 'income',\n"," '20',\n"," 'comes',\n"," 'theft',\n"," 'generates',\n"," 'further',\n"," '18',\n"," '11',\n"," 'illegal',\n"," '7',\n"," 'important',\n"," 'detail',\n"," 'highway',\n"," 'robbery',\n"," 'trucks',\n"," 'articulated',\n"," 'lorries',\n"," '90',\n"," 'committed',\n"," 'favourite',\n"," 'loads',\n"," 'clothing',\n"," 'electronic',\n"," 'equipment',\n"," 'friendly',\n"," 'stores',\n"," 'clandestine',\n"," 'supermarkets',\n"," 'value',\n"," 'load',\n"," 'minimum',\n"," 'recovered',\n"," 'gangs',\n"," 'gang',\n"," 'charged',\n"," '1600',\n"," 'thefts',\n"," 'hijacking',\n"," 'cargoes',\n"," 'problem',\n"," 'find',\n"," 'resources',\n"," 'combat',\n"," '250000',\n"," 'carabinieri',\n"," 'highest',\n"," 'capita',\n"," 'population',\n"," 'poorly',\n"," 'rivals',\n"," 'parties',\n"," 'south',\n"," 'entire',\n"," 'economic',\n"," 'dominated',\n"," 'funding',\n"," 'encourages',\n"," 'permits',\n"," 'fecund',\n"," 'politics',\n"," 'changes',\n"," 'thrive',\n"," 'beyond',\n"," 'legal',\n"," 'system',\n"," 'unwieldy',\n"," 'easily',\n"," 'escape',\n"," 'either',\n"," 'procedural',\n"," 'devices',\n"," 'proof',\n"," 'security',\n"," 'detailed',\n"," 'lists',\n"," 'final',\n"," 'resort',\n"," 'power',\n"," 'bribe',\n"," 'intimidate',\n"," 'judiciary',\n"," 'obliged',\n"," 'seek',\n"," 'volunteers',\n"," 'investigation',\n"," '34',\n"," 'kingpins',\n"," 'including',\n"," 'senior',\n"," 'executive',\n"," 'colombias',\n"," 'national',\n"," 'costa',\n"," 'rica',\n"," 'huge',\n"," 'cache',\n"," 'called',\n"," 'operation',\n"," 'green',\n"," 'ice',\n"," 'took',\n"," 'plan',\n"," 'arrests',\n"," 'carried',\n"," 'jointly',\n"," 'intricate',\n"," 'methods',\n"," 'couriers',\n"," 'ships',\n"," 'real',\n"," 'companies',\n"," 'many',\n"," 'countries',\n"," 'smuggle',\n"," 'then',\n"," 'sent',\n"," 'back',\n"," 'via',\n"," 'austria',\n"," 'switzerland',\n"," 'significant',\n"," 'jose',\n"," 'tony',\n"," 'pope',\n"," 'duran',\n"," '38',\n"," 'distributor',\n"," 'alleged',\n"," 'bar',\n"," 'uk',\n"," 'investigators',\n"," 'men',\n"," 'believed',\n"," 'americans',\n"," 'area',\n"," 'friday',\n"," 'street',\n"," '7m',\n"," 'during',\n"," 'contribution',\n"," 'customs',\n"," '25m',\n"," 'what',\n"," 'locations',\n"," 'garage',\n"," 'held',\n"," 'questioned',\n"," 'undisclosed',\n"," 'location',\n"," 'douglas',\n"," 'investigator',\n"," 'night',\n"," 'major',\n"," 'blow',\n"," 'we',\n"," 'managed',\n"," 'lot',\n"," 'away',\n"," 'make',\n"," 'very',\n"," 'operate',\n"," 'british',\n"," 'say',\n"," 'nigerians',\n"," 'trying',\n"," 'any',\n"," 'nationals',\n"," 'estimate',\n"," 'sophisticated',\n"," 'nigerian',\n"," 'rings',\n"," 'account',\n"," 'seizures',\n"," 'ports',\n"," 'entry',\n"," 'multimillion',\n"," 'dollar',\n"," 'tightly',\n"," 'lagos',\n"," 'transit',\n"," 'station',\n"," 'exploit',\n"," 'countrys',\n"," 'loose',\n"," 'porous',\n"," 'borders',\n"," 'force',\n"," 'notorious',\n"," 'corruption',\n"," 'hundreds',\n"," 'asian',\n"," 'mostly',\n"," 'india',\n"," 'thailand',\n"," 'golden',\n"," 'triangle',\n"," 'pass',\n"," 'storage',\n"," 'repackaging',\n"," 'nigeria',\n"," 'largest',\n"," 'supplier',\n"," 'cannabis',\n"," 'ghana',\n"," 'exporting',\n"," 'white',\n"," '1992',\n"," 'increased',\n"," 'impossible',\n"," 'know',\n"," 'percentage',\n"," 'smuggled',\n"," 'estimates',\n"," 'vary',\n"," 'difficulty',\n"," 'stem',\n"," 'large',\n"," 'quantity',\n"," 'sophistication',\n"," 'greatest',\n"," 'officer',\n"," 'western',\n"," 'prison',\n"," 'sentences',\n"," 'deterrent',\n"," 'potential',\n"," 'smugglers',\n"," 'meals',\n"," 'television',\n"," 'chance',\n"," 'get',\n"," 'education',\n"," 'earn',\n"," 'pocket',\n"," 'making',\n"," '30',\n"," 'suddenly',\n"," 'offered',\n"," '5000',\n"," 'run',\n"," 'connection',\n"," 'surfaced',\n"," 'around',\n"," 'grown',\n"," 'economy',\n"," 'once',\n"," 'rich',\n"," 'oil',\n"," 'producer',\n"," 'plummeted',\n"," 'widespread',\n"," 'service',\n"," 'absence',\n"," 'tough',\n"," 'laws',\n"," 'nigerias',\n"," 'commissioner',\n"," '13',\n"," 'judges',\n"," 'policeman',\n"," 'implicated',\n"," 'release',\n"," 'suspects',\n"," 'awaiting',\n"," 'freed',\n"," 'using',\n"," 'forged',\n"," 'bonds',\n"," 'orders',\n"," 'accusations',\n"," 'paid',\n"," 'agencys',\n"," 'fell',\n"," 'sharply',\n"," 'far',\n"," ...]"]},"execution_count":119,"metadata":{},"output_type":"execute_result"}],"source":["#Read data\n","a_file = open(\"output_data/UNK 5-55/used_words.pkl\", \"rb\")\n","used_words = pickle.load(a_file)\n","a_file.close()\n","used_words"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[{"data":{"text/plain":["22734"]},"execution_count":120,"metadata":{},"output_type":"execute_result"}],"source":["len(used_words)"]},{"cell_type":"code","execution_count":121,"metadata":{"id":"rxMvj3Jgcqd-"},"outputs":[],"source":["tokenized_sentences_unk = []\n","for sentence in tokenized_sentences:\n","    l_replace_all = ['<UNK>' if word not in used_words else word for word in sentence]\n","    tokenized_sentences_unk.append(l_replace_all)\n","\n","#tokenized_sentences_unk[0]"]},{"cell_type":"code","execution_count":122,"metadata":{"id":"mzeUQ1J_cqd_"},"outputs":[],"source":["# Save test sentences with the <UNK> tags \n","filename = os.path.join(\"output_data/UNK 5-55/\", \"test_sentences_unk.pkl\")\n","with open(filename, \"wb\") as file: \n","    pickle.dump(tokenized_sentences_unk, file)"]},{"cell_type":"markdown","metadata":{"id":"lGXwqgx1cqd_"},"source":["## Generate N-Grams"]},{"cell_type":"markdown","metadata":{"id":"TZ_VcHovcqd_"},"source":["### Unigrams"]},{"cell_type":"markdown","metadata":{"id":"SNwL6VUIcqd_"},"source":["For unigrams, we don't take into account the sentences, so we are just getting the single tokens and creating a dictionary"]},{"cell_type":"code","execution_count":123,"metadata":{"id":"RlbEBbXDcqd_"},"outputs":[],"source":["#Join elements of the list tokenized_sentences_unk (that contains each sentence unigrams) into a single list\n","unigrams = list(itertools.chain.from_iterable(tokenized_sentences_unk))\n","#unigrams"]},{"cell_type":"markdown","metadata":{"id":"zsMz6Torcqd_"},"source":["#### Generate Unigram Dictionary"]},{"cell_type":"code","execution_count":124,"metadata":{"id":"tIlgBWVPcqd_","outputId":"b8d87dcb-1ba6-4e34-ec4a-9a1394e31ac1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 20 unigrams and frequency: \n"," [('the', 11299), ('<UNK>', 11210), ('of', 5254), ('to', 4868), ('a', 4264), ('and', 4259), ('in', 3905), ('that', 1888), ('for', 1875), ('is', 1760), ('on', 1260), ('by', 1204), ('it', 1166), ('was', 1119), ('said', 1099), ('as', 1076), ('are', 1007), ('with', 997), ('he', 976), ('at', 941)]\n"]}],"source":["from nltk.probability import FreqDist\n","\n","unigrams_dict = FreqDist(unigrams)\n","unigrams_dict_top_20 = unigrams_dict.most_common(20)\n","print(\"Top 20 unigrams and frequency: \\n\", unigrams_dict_top_20)"]},{"cell_type":"code","execution_count":125,"metadata":{"id":"tuzAnAPocqd_"},"outputs":[],"source":["#Convert FreqDist to disctionary\n","unigrams_dict = dict(unigrams_dict)\n","\n","# Add <s> and </s> tags to the unigram dict\n","num_sentences = len(tokenized_sentences_unk)\n","unigrams_dict['<s>'] = num_sentences\n","unigrams_dict['</s>'] = num_sentences"]},{"cell_type":"markdown","metadata":{"id":"cjpPIh-2cqd_"},"source":["### Bigrams"]},{"cell_type":"markdown","metadata":{"id":"c6gesom1cqd_"},"source":["#### Add Padding Symbols to Sentences"]},{"cell_type":"code","execution_count":126,"metadata":{"id":"sWA2Ozhncqd_"},"outputs":[],"source":["from nltk.util import pad_sequence\n","\n","bi_tokens_padding = []\n","\n","for sentence in tokenized_sentences_unk:\n","    e = list(pad_sequence(sentence,\n","                     pad_left=True, left_pad_symbol=\"<s>\",\n","                     pad_right=True, right_pad_symbol=\"</s>\",\n","                     n=2))\n","    bi_tokens_padding.append(e)\n","\n","#bi_tokens_padding[0]"]},{"cell_type":"markdown","metadata":{"id":"NUwuFnWBcqd_"},"source":["#### Generate Bigram Dictionary"]},{"cell_type":"code","execution_count":127,"metadata":{"id":"N3x51_4lcqd_"},"outputs":[],"source":["bigrams = []\n","for sentence in bi_tokens_padding:\n","    bigrams.append(list(nltk.bigrams(sentence)))\n","\n","#bigrams"]},{"cell_type":"code","execution_count":128,"metadata":{"id":"nyYPrJ8Ecqd_"},"outputs":[],"source":["#Join elements of the list bigrams (that contains each sentence bigrams) into a single list\n","bigrams = list(itertools.chain.from_iterable(bigrams))\n","#bigrams"]},{"cell_type":"code","execution_count":129,"metadata":{"id":"elurE4YYcqd_","outputId":"83e8a0b0-328d-4437-bdc3-8d26c35159c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 20 bigrams and frequency: \n"," [(('of', 'the'), 1355), (('<s>', 'the'), 1221), (('<UNK>', '<UNK>'), 1170), (('the', '<UNK>'), 1062), (('in', 'the'), 963), (('<UNK>', '</s>'), 826), (('<s>', '<UNK>'), 572), (('<UNK>', 'and'), 568), (('a', '<UNK>'), 488), (('to', 'the'), 466), (('of', '<UNK>'), 456), (('and', '<UNK>'), 429), (('on', 'the'), 387), (('for', 'the'), 358), (('said', '</s>'), 343), (('<s>', 'but'), 333), (('<s>', 'in'), 326), (('<UNK>', 'of'), 317), (('by', 'the'), 295), (('<UNK>', 'the'), 293)]\n"]}],"source":["bigrams_dict = FreqDist(bigrams)\n","bigrams_dict_top_20 = bigrams_dict.most_common(20)\n","print(\"Top 20 bigrams and frequency: \\n\", bigrams_dict_top_20)"]},{"cell_type":"code","execution_count":130,"metadata":{"id":"QjLEO8MBcqd_"},"outputs":[],"source":["#Convert FreqDist to disctionary\n","bigrams_dict = dict(bigrams_dict)\n","\n","# Add <s><s> and </s></s> bigrams to the bigram dict\n","num_sentences = len(tokenized_sentences_unk)\n","bigrams_dict[('<s>', '<s>')] = num_sentences\n","bigrams_dict[('</s>', '</s>')] = num_sentences\n","#bigrams_dict"]},{"cell_type":"markdown","metadata":{"id":"JAFDESkqcqd_"},"source":["### Trigrams"]},{"cell_type":"markdown","metadata":{"id":"Rv7Xu3rccqd_"},"source":["#### Add Padding Symbols to Sentences"]},{"cell_type":"code","execution_count":131,"metadata":{"id":"YiUdXNdScqeA"},"outputs":[],"source":["tri_tokens_padding = []\n","\n","for sentence in bi_tokens_padding:\n","    e = list(pad_sequence(sentence,\n","                     pad_left=True, left_pad_symbol=\"<s>\",\n","                     pad_right=True, right_pad_symbol=\"</s>\",\n","                     n=2))\n","    tri_tokens_padding.append(e)\n","\n","#tri_tokens_padding[0]"]},{"cell_type":"markdown","metadata":{"id":"uMjO5GiqcqeA"},"source":["#### Generate Trigram Dictionary"]},{"cell_type":"code","execution_count":132,"metadata":{"id":"LHd7muI2cqeA"},"outputs":[],"source":["trigrams = []\n","for sentence in tri_tokens_padding:\n","    trigrams.append(list(nltk.trigrams(sentence)))\n","\n","#trigrams[0]"]},{"cell_type":"code","execution_count":133,"metadata":{"id":"-u4KA8n0cqeA"},"outputs":[],"source":["#Join elements of the list trigrams (that contains each sentence trigrams) into a single list\n","trigrams = list(itertools.chain.from_iterable(trigrams))\n","#trigrams"]},{"cell_type":"code","execution_count":134,"metadata":{"id":"JkgYZJgqcqeA","outputId":"b88081f6-f827-4117-a115-864a665a20af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 20 triigrams and frequency: \n"," [(('<s>', '<s>', 'the'), 1221), (('<UNK>', '</s>', '</s>'), 826), (('<s>', '<s>', '<UNK>'), 572), (('said', '</s>', '</s>'), 343), (('<s>', '<s>', 'but'), 333), (('<s>', '<s>', 'in'), 326), (('<s>', '<s>', 'it'), 285), (('<s>', '<s>', 'he'), 237), (('<s>', '<s>', 'i'), 227), (('<UNK>', '<UNK>', '<UNK>'), 189), (('<s>', '<s>', 'mr'), 163), (('<s>', '<s>', 'a'), 156), (('<UNK>', 'and', '<UNK>'), 152), (('?', '</s>', '</s>'), 146), (('of', 'the', '<UNK>'), 138), (('<s>', '<s>', 'we'), 136), (('<s>', '<s>', 'they'), 123), (('<s>', '<s>', 'this'), 122), (('<s>', 'the', '<UNK>'), 112), (('<UNK>', 'said', '</s>'), 105)]\n"]}],"source":["trigrams_dict = FreqDist(trigrams)\n","trigrams_dict_top_20 = trigrams_dict.most_common(20)\n","print(\"Top 20 triigrams and frequency: \\n\", trigrams_dict_top_20)"]},{"cell_type":"code","execution_count":135,"metadata":{"id":"lo8iyDgEcqeA"},"outputs":[],"source":["#Convert FreqDist to disctionary\n","trigrams_dict = dict(trigrams_dict)\n","\n","# Add <s><s><s> and </s></s></s> trigrams to the trigram dict\n","num_sentences = len(tokenized_sentences_unk)\n","trigrams_dict[('<s>', '<s>', '<s>')] = num_sentences\n","trigrams_dict[('</s>', '</s>', '</s>')] = num_sentences\n","\n","#trigrams_dict"]},{"cell_type":"markdown","metadata":{"id":"ReQaKCLjcqeA"},"source":["### Four-grams"]},{"cell_type":"markdown","metadata":{"id":"K91VQsTdcqeA"},"source":["#### Add Padding Symbols to Sentences"]},{"cell_type":"code","execution_count":136,"metadata":{"id":"wIJIoEFecqeA"},"outputs":[],"source":["four_tokens_padding = []\n","\n","for sentence in tri_tokens_padding:\n","    e = list(pad_sequence(sentence,\n","                     pad_left=True, left_pad_symbol=\"<s>\",\n","                     pad_right=True, right_pad_symbol=\"</s>\",\n","                     n=2))\n","    four_tokens_padding.append(e)\n","\n","#four_tokens_padding[0]"]},{"cell_type":"markdown","metadata":{"id":"SYEeLdaycqeA"},"source":["#### Generate Four-gram Dictionary"]},{"cell_type":"code","execution_count":137,"metadata":{"id":"oPhZkh75cqeA"},"outputs":[],"source":["from nltk.util import ngrams\n","\n","fourgrams = []\n","for sentence in four_tokens_padding:\n","    fourgrams.append( list(ngrams(sentence, 4)) )\n","\n","#fourgrams[0]"]},{"cell_type":"code","execution_count":138,"metadata":{"id":"b89kN2QbcqeA"},"outputs":[],"source":["#Join elements of the list fourgrams (that contains each sentence fourgrams) into a single list\n","fourgrams = list(itertools.chain.from_iterable(fourgrams))\n","#fourgrams"]},{"cell_type":"code","execution_count":139,"metadata":{"id":"G_deOVOOcqeA","outputId":"ce62c219-120b-481b-9e68-5905a613c159"},"outputs":[{"name":"stdout","output_type":"stream","text":["Top 20 fourgrams and frequency: \n"," [(('<s>', '<s>', '<s>', 'the'), 1221), (('<UNK>', '</s>', '</s>', '</s>'), 826), (('<s>', '<s>', '<s>', '<UNK>'), 572), (('said', '</s>', '</s>', '</s>'), 343), (('<s>', '<s>', '<s>', 'but'), 333), (('<s>', '<s>', '<s>', 'in'), 326), (('<s>', '<s>', '<s>', 'it'), 285), (('<s>', '<s>', '<s>', 'he'), 237), (('<s>', '<s>', '<s>', 'i'), 227), (('<s>', '<s>', '<s>', 'mr'), 163), (('<s>', '<s>', '<s>', 'a'), 156), (('?', '</s>', '</s>', '</s>'), 146), (('<s>', '<s>', '<s>', 'we'), 136), (('<s>', '<s>', '<s>', 'they'), 123), (('<s>', '<s>', '<s>', 'this'), 122), (('<s>', '<s>', 'the', '<UNK>'), 112), (('<UNK>', 'said', '</s>', '</s>'), 105), (('<s>', '<s>', '<s>', 'and'), 101), (('<s>', '<s>', '<s>', 'there'), 98), (('<s>', '<s>', '<s>', 'if'), 98)]\n"]}],"source":["fourgrams_dict = FreqDist(fourgrams)\n","fourgrams_dict_top_20 = fourgrams_dict.most_common(20)\n","print(\"Top 20 fourgrams and frequency: \\n\", fourgrams_dict_top_20)"]},{"cell_type":"code","execution_count":140,"metadata":{"id":"KH1hoR10cqeA"},"outputs":[],"source":["#Convert FreqDist to disctionary\n","fourgrams_dict = dict(fourgrams_dict)\n","#fourgrams_dict"]},{"cell_type":"markdown","metadata":{"id":"zhdA12GMcqeA"},"source":["### Export N-Gram Dictionaries (in pkl files)"]},{"cell_type":"code","execution_count":141,"metadata":{"id":"MqzCrucmcqeA"},"outputs":[],"source":["import pickle\n","output_folder = \"output_data/UNK 5-55/\"\n","unigram_dictionary_file = \"unigram_dictionary_test.pkl\"\n","bigram_dictionary_file = \"bigram_dictionary_test.pkl\"\n","trigram_dictionary_file = \"trigram_dictionary_test.pkl\"\n","fourgram_dictionary_file = \"fourgram_dictionary_test.pkl\""]},{"cell_type":"code","execution_count":142,"metadata":{"id":"PrEisoLBcqeA"},"outputs":[],"source":["def export_dictionary(dict, output_file):\n","    a_file = open(output_folder + output_file, \"wb\")\n","    pickle.dump(dict, a_file)\n","    a_file.close()"]},{"cell_type":"code","execution_count":143,"metadata":{"id":"zbo3EHQAcqeA"},"outputs":[],"source":["#Export dictionaries\n","export_dictionary(unigrams_dict, unigram_dictionary_file)\n","export_dictionary(bigrams_dict, bigram_dictionary_file)\n","export_dictionary(trigrams_dict, trigram_dictionary_file)\n","export_dictionary(fourgrams_dict, fourgram_dictionary_file)"]},{"cell_type":"markdown","metadata":{"id":"un1sNCBbcqeA"},"source":["#### How to Import pkl data"]},{"cell_type":"code","execution_count":144,"metadata":{"id":"PrLkYRc9cqeB"},"outputs":[],"source":["#Inverse process: Read data\n","a_file = open(output_folder + bigram_dictionary_file, \"rb\")\n","output = pickle.load(a_file)\n","a_file.close()\n","#output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwTUP3vucqeB"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"4a68d6c0066e8dcb68188937774911e93882d11496a351f23b3ed5d4f27effc2"}}},"nbformat":4,"nbformat_minor":0}
