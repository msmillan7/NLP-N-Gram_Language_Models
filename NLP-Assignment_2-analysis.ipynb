{"cells":[{"cell_type":"markdown","metadata":{"id":"2t4AxK5JRIPe"},"source":["# Pipeline 2: Analysis"]},{"cell_type":"markdown","metadata":{"id":"bP-gHtOHRIPh"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"373yQzP8RIPh"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import math\n","\n","import nltk\n","from nltk.util import pad_sequence\n","from nltk.util import ngrams\n","\n","import itertools\n","import pickle\n","\n","from itertools import permutations"]},{"cell_type":"markdown","metadata":{"id":"rXlXz7BYRIPj"},"source":["## Load pkl files"]},{"cell_type":"markdown","metadata":{"id":"NFTSfO0ERIPj"},"source":["Load n-gram training dictionaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L7GHApMIRIPj"},"outputs":[],"source":["def load_pkl(folder, file):\n","    \"\"\"Helper function to load a pkl file\"\"\"\n","    filename = os.path.join(folder, file)\n","    with open(filename, \"rb\") as file: \n","        file_contents = pickle.load(file)\n","    return(file_contents)"]},{"cell_type":"markdown","metadata":{"id":"Wpfoi-FERIPk"},"source":["#### Training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4k7MSqEsRIPk"},"outputs":[],"source":["data_folder = \"output_data/UNK 5-55/\"\n","unigram_pkl = \"unigram_dictionary_training.pkl\"\n","bigram_pkl = \"bigram_dictionary_training.pkl\"\n","trigram_pkl = \"trigram_dictionary_training.pkl\"\n","fourgram_pkl = \"fourgram_dictionary_training.pkl\"\n","training_sentences_unk=\"training_sentences_unk.pkl\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Mcs6o6uRIPl"},"outputs":[],"source":["unigram_dict = load_pkl(data_folder, unigram_pkl)\n","bigram_dict = load_pkl(data_folder, bigram_pkl)\n","trigram_dict = load_pkl(data_folder, trigram_pkl)\n","fourgram_dict = load_pkl(data_folder, fourgram_pkl)"]},{"cell_type":"markdown","metadata":{"id":"gWAGf_GoRIPl"},"source":["Create a dictionary *ngram_dicts* that combines the unigram, bigram, trigram, and fourgram dicts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAd9CavnRIPm"},"outputs":[],"source":["ngram_dict = unigram_dict | bigram_dict | trigram_dict | fourgram_dict"]},{"cell_type":"markdown","metadata":{"id":"1e5i3gWBRIPm"},"source":["#### Testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMLIfdv5RIPm"},"outputs":[],"source":["#Data for testing\n","unigram_pkl_test = \"unigram_dictionary_test.pkl\"\n","bigram_pkl_test = \"bigram_dictionary_test.pkl\"\n","trigram_pkl_test = \"trigram_dictionary_test.pkl\"\n","fourgram_pkl_test = \"fourgram_dictionary_test.pkl\"\n","test_sentences_unk=\"test_sentences_unk.pkl\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zd-c_U0zRIPn"},"outputs":[],"source":["unigram_dict_test = load_pkl(data_folder, unigram_pkl_test)\n","bigram_dict_test = load_pkl(data_folder, bigram_pkl_test)\n","trigram_dict_test = load_pkl(data_folder, trigram_pkl_test)\n","fourgram_dict_test = load_pkl(data_folder, fourgram_pkl_test)"]},{"cell_type":"markdown","metadata":{"id":"cHKD5uQsRIPn"},"source":["## Helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amxpHDvhRIPn"},"outputs":[],"source":["def ngram_probability(ngram, k = None): \n","    \"\"\" Computes the probability of the given ngram. \n","    \n","    Parameters\n","    -------------\n","    ngram: tple. representing an n-gram of length n>=2\n","    k: float or None. If None is provided, no smoothing is applied. \n","        If a float is provided, add-k smoothing is applied.\n","    \n","    Return\n","    -------------\n","    The probability of the ngram (with or without add-k smoothing)\n","\n","    \"\"\"\n","    \n","    n = len(ngram)\n","    \n","    # Obtain prefix\n","    if n > 2: \n","        prefix = ngram[:(n-1)]\n","    elif n == 2:\n","        prefix = ngram[0]\n","    else: \n","        print(\"ngram must be of length 2 or greater\")\n","            \n","    # No smoothing applied\n","    if k is None: \n","        probability = ngram_dict[ngram]/float(ngram_dict[prefix])\n","        \n","    # Apply add-k smoothing\n","    else:\n","        V = float(len(unigram_dict)) # Vocabulary size\n","        probability = (ngram_dict[ngram] + k)/(ngram_dict[prefix] + (k*V))\n","    \n","    return(probability)"]},{"cell_type":"markdown","metadata":{"id":"6svq2mbMRIPn"},"source":["## I. Summary statistics for the training/ testing datasets"]},{"cell_type":"markdown","metadata":{"id":"nGc9g7_cRIPo"},"source":["#### Training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkB4f18PRIPo","outputId":"d4dfaa8c-2a15-4e3f-8755-57c6c769ffc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of tokens in the training set: 859703\n"]}],"source":["print(\"Total number of tokens in the training set:\", np.sum(list(unigram_dict.values())))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4d8SvNORIPp","outputId":"18d4795a-5ebf-4586-e885-41510261b7de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of UNK tags in training set: 52997\n"]}],"source":["print(\"Number of UNK tags in training set:\", unigram_dict['<UNK>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIdioyacRIPp","outputId":"179dfc59-76dc-407a-bd40-b83f940d9f23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training data statistics:\n","Number unique unigrams:  10247\n","Number unique bigrams:  264407\n","Number unique trigrams:  567807\n","Number unique 4-grams:  711575\n"]}],"source":["print(\"Training data statistics:\")\n","print(\"Number unique unigrams: \", len(unigram_dict))\n","print(\"Number unique bigrams: \", len(bigram_dict))\n","print(\"Number unique trigrams: \", len(trigram_dict))\n","print(\"Number unique 4-grams: \", len(fourgram_dict))"]},{"cell_type":"markdown","metadata":{"id":"vNjOK3o8RIPp"},"source":["#### Testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vjCWU1ryRIPp","outputId":"5437dea2-2799-4cf9-9f4a-14d1807a7eff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of tokens in the testing set: 199642\n"]}],"source":["print(\"Total number of tokens in the testing set:\", np.sum(list(unigram_dict_test.values())))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soO6t_KNRIPq","outputId":"3342ef8c-fa6c-491c-cfeb-3dd8c244137b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of UNK tags in testing set: 11210\n"]}],"source":["print(\"Number of UNK tags in testing set:\", unigram_dict_test['<UNK>'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfTWtHBpRIPq","outputId":"f77c58c5-b4da-4e29-e3bb-3978c75051fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing data statistics:\n","Number unique unigrams:  10767\n","Number unique bigrams:  90140\n","Number unique trigrams:  154378\n","Number unique 4-grams:  179327\n"]}],"source":["# Testing data\n","print(\"Testing data statistics:\")\n","print(\"Number unique unigrams: \", len(unigram_dict_test))\n","print(\"Number unique bigrams: \", len(bigram_dict_test))\n","print(\"Number unique trigrams: \", len(trigram_dict_test))\n","print(\"Number unique 4-grams: \", len(fourgram_dict_test))"]},{"cell_type":"markdown","metadata":{"id":"csCgCjl0RIPq"},"source":["## II. Code for generating tables in Section 3: Analysis"]},{"cell_type":"markdown","metadata":{"id":"EE_v2_GiRIPq"},"source":["#### Computation of n-gram counts (smoothed and unsmoothed) and probabilities (smoothed and unsmoothed) "]},{"cell_type":"markdown","metadata":{"id":"cbmYOHWORIPq"},"source":["Bigram computations (counts and probabilities)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1hv_Ey4bRIPq"},"outputs":[],"source":["bigram_unsmoothed_count_dict = bigram_dict.copy()\n","bigram_smoothed_count_dict = {}\n","bigram_unsmoothed_prob_dict = {}\n","bigram_smoothed_prob_dict = {}\n","\n","# Compute the smoothed counts and smoothed and unsmoothed probabilities\n","for bigram in bigram_dict.keys(): \n","    bigram_smoothed_count_dict[bigram] = bigram_dict[bigram] + 1\n","    bigram_unsmoothed_prob_dict[bigram] = ngram_probability(bigram, k=None)\n","    bigram_smoothed_prob_dict[bigram] = ngram_probability(bigram, k=1)"]},{"cell_type":"markdown","metadata":{"id":"HKF0eVtaRIPq"},"source":["Trigram probability computations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7zhTyCuRIPq"},"outputs":[],"source":["trigram_smoothed_prob_dict = {}\n","trigram_unsmoothed_prob_dict = {}\n","\n","# Compute the smoothed smoothed and unsmoothed probabilities\n","for trigram in trigram_dict.keys(): \n","    trigram_unsmoothed_prob_dict[trigram] = ngram_probability(trigram, k=None)\n","    trigram_smoothed_prob_dict[trigram] = ngram_probability(trigram, k=1)"]},{"cell_type":"markdown","metadata":{"id":"TS_WEuYQRIPq"},"source":["Fourgram probability computations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqW6ALQiRIPr"},"outputs":[],"source":["fourgram_smoothed_prob_dict = {}\n","fourgram_unsmoothed_prob_dict = {}\n","\n","# Compute the smoothed smoothed and unsmoothed probabilities\n","for fourgram in fourgram_dict.keys(): \n","    fourgram_unsmoothed_prob_dict[fourgram] = ngram_probability(fourgram, k=None)\n","    fourgram_smoothed_prob_dict[fourgram] = ngram_probability(fourgram, k=1)"]},{"cell_type":"markdown","metadata":{"id":"9YU-TH1dRIPr"},"source":["#### Table Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yElobd1DRIPr"},"outputs":[],"source":["# Dictionary of unigrams sorted in descending order by frequency \n","sorted_unigrams_tpls = sorted(unigram_dict.items(), key=lambda item: item[1], reverse = True)\n","sorted_unigrams_dct = {k: v for k, v in sorted_unigrams_tpls}\n","# sorted_unigrams_dct"]},{"cell_type":"markdown","metadata":{"id":"LEfydxL1RIPr"},"source":["We selected 8 unigrams that appeared frequently in the `sorted_unigrams_dct`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WExig50WRIPr"},"outputs":[],"source":["# 8 frequent occurring unigrams in the vocabulary\n","words = ['the', 'people', 'said', 'of', 'last', 'two', 'financial', 'years']\n","\n","# Obtain all permutations of length-2 from the 8-word list \n","all_tples =  list(permutations(words, 2))\n","for word in words:\n","    all_tples.append((word, word))"]},{"cell_type":"markdown","metadata":{"id":"Vu-GfhMHRIPr"},"source":["Obtain the bigram counts and probabilities with and without smoothing for the given set of 8 words to be used for generating the tables in figure XX and figure XX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuTryHrNRIPr"},"outputs":[],"source":["tble_unsmoothed_counts = {}\n","tble_smoothed_counts = {}\n","tble_unsmoothed_prob = {}\n","tble_smoothed_prob = {}\n","for tple in all_tples: \n","    tble_unsmoothed_counts[tple] = bigram_unsmoothed_count_dict.get(tple, 0)\n","    tble_smoothed_counts[tple] = bigram_smoothed_count_dict.get(tple, 1)\n","    tble_unsmoothed_prob[tple] = bigram_unsmoothed_prob_dict.get(tple, 0)\n","    tble_smoothed_prob[tple] = bigram_smoothed_prob_dict.get(tple, 0)"]},{"cell_type":"markdown","metadata":{"id":"AvN18RE_RIPr"},"source":["Generate the tables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpx62TyhRIPr"},"outputs":[],"source":["def generate_tble(words, dct): \n","    \"\"\" Create a table containing the frequency or probability of a given bigram based on the provided words and dictionary \"\"\"\n","    df = pd.DataFrame(0, columns=words, index=words)\n","    \n","    # Populate the dataframe with counts or probabilities\n","    for (key, value) in dct.items():\n","        df.at[key[0], key[1]] = round(dct[key], 5)\n","        \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtQNsdVgRIPs","outputId":"e1263801-328f-426f-cc45-b87b10179a27"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>the</th>\n","      <th>people</th>\n","      <th>said</th>\n","      <th>of</th>\n","      <th>last</th>\n","      <th>two</th>\n","      <th>financial</th>\n","      <th>years</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>the</th>\n","      <td>13</td>\n","      <td>76</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>212</td>\n","      <td>263</td>\n","      <td>445</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>people</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>25</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>said</th>\n","      <td>424</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>24</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>of</th>\n","      <td>6021</td>\n","      <td>94</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>32</td>\n","      <td>49</td>\n","      <td>13</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>last</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>1</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>two</th>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>36</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>124</td>\n","    </tr>\n","    <tr>\n","      <th>financial</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>years</th>\n","      <td>33</td>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>93</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            the  people  said  of  last  two  financial  years\n","the          13      76     0   0   212  263        445     27\n","people        5       0     4  25     0    1          0      0\n","said        424       2     0  17    24    3          0      0\n","of         6021      94     1   1    32   49         13      8\n","last          1       0     1   3     0   16          1     30\n","two           0       7     1  36     0    0          1    124\n","financial     0       0     0   0     0    0          0      1\n","years        33       1    11  93     0    0          0      0"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Table of unsmoothed bigram counts\n","generate_tble(words, tble_unsmoothed_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faGsQqDwRIPs","outputId":"db95f0c6-a554-4a65-c8d2-79abbe8a098f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>the</th>\n","      <th>people</th>\n","      <th>said</th>\n","      <th>of</th>\n","      <th>last</th>\n","      <th>two</th>\n","      <th>financial</th>\n","      <th>years</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>the</th>\n","      <td>14</td>\n","      <td>77</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>213</td>\n","      <td>264</td>\n","      <td>446</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>people</th>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>26</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>said</th>\n","      <td>425</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>18</td>\n","      <td>25</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>of</th>\n","      <td>6022</td>\n","      <td>95</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>33</td>\n","      <td>50</td>\n","      <td>14</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>last</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>2</td>\n","      <td>31</td>\n","    </tr>\n","    <tr>\n","      <th>two</th>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>37</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>125</td>\n","    </tr>\n","    <tr>\n","      <th>financial</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>years</th>\n","      <td>34</td>\n","      <td>2</td>\n","      <td>12</td>\n","      <td>94</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            the  people  said  of  last  two  financial  years\n","the          14      77     1   1   213  264        446     28\n","people        6       1     5  26     1    2          1      1\n","said        425       3     1  18    25    4          1      1\n","of         6022      95     2   2    33   50         14      9\n","last          2       1     2   4     1   17          2     31\n","two           1       8     2  37     1    1          2    125\n","financial     1       1     1   1     1    1          1      2\n","years        34       2    12  94     1    1          1      1"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Table of smoothed bigram counts\n","generate_tble(words, tble_smoothed_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUfML4H5RIPs","outputId":"68784e3e-372b-48c3-c519-691622bf1e1b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>the</th>\n","      <th>people</th>\n","      <th>said</th>\n","      <th>of</th>\n","      <th>last</th>\n","      <th>two</th>\n","      <th>financial</th>\n","      <th>years</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>the</th>\n","      <td>0.00026</td>\n","      <td>0.00151</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00423</td>\n","      <td>0.00524</td>\n","      <td>0.00887</td>\n","      <td>0.00054</td>\n","    </tr>\n","    <tr>\n","      <th>people</th>\n","      <td>0.00437</td>\n","      <td>0.00000</td>\n","      <td>0.00350</td>\n","      <td>0.02185</td>\n","      <td>0.00000</td>\n","      <td>0.00087</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>said</th>\n","      <td>0.10090</td>\n","      <td>0.00048</td>\n","      <td>0.00000</td>\n","      <td>0.00405</td>\n","      <td>0.00571</td>\n","      <td>0.00071</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>of</th>\n","      <td>0.25123</td>\n","      <td>0.00392</td>\n","      <td>0.00004</td>\n","      <td>0.00004</td>\n","      <td>0.00134</td>\n","      <td>0.00204</td>\n","      <td>0.00054</td>\n","      <td>0.00033</td>\n","    </tr>\n","    <tr>\n","      <th>last</th>\n","      <td>0.00083</td>\n","      <td>0.00000</td>\n","      <td>0.00083</td>\n","      <td>0.00248</td>\n","      <td>0.00000</td>\n","      <td>0.01322</td>\n","      <td>0.00083</td>\n","      <td>0.02479</td>\n","    </tr>\n","    <tr>\n","      <th>two</th>\n","      <td>0.00000</td>\n","      <td>0.00554</td>\n","      <td>0.00079</td>\n","      <td>0.02850</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00079</td>\n","      <td>0.09818</td>\n","    </tr>\n","    <tr>\n","      <th>financial</th>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00166</td>\n","    </tr>\n","    <tr>\n","      <th>years</th>\n","      <td>0.02322</td>\n","      <td>0.00070</td>\n","      <td>0.00774</td>\n","      <td>0.06545</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               the   people     said       of     last      two  financial  \\\n","the        0.00026  0.00151  0.00000  0.00000  0.00423  0.00524    0.00887   \n","people     0.00437  0.00000  0.00350  0.02185  0.00000  0.00087    0.00000   \n","said       0.10090  0.00048  0.00000  0.00405  0.00571  0.00071    0.00000   \n","of         0.25123  0.00392  0.00004  0.00004  0.00134  0.00204    0.00054   \n","last       0.00083  0.00000  0.00083  0.00248  0.00000  0.01322    0.00083   \n","two        0.00000  0.00554  0.00079  0.02850  0.00000  0.00000    0.00079   \n","financial  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000    0.00000   \n","years      0.02322  0.00070  0.00774  0.06545  0.00000  0.00000    0.00000   \n","\n","             years  \n","the        0.00054  \n","people     0.00000  \n","said       0.00000  \n","of         0.00033  \n","last       0.02479  \n","two        0.09818  \n","financial  0.00166  \n","years      0.00000  "]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Table of unsmoothed bigram probabilities\n","generate_tble(words, tble_unsmoothed_prob)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"snyjfIXzRIPs","outputId":"24f79741-e3a2-440d-aaf4-04ea414ac2d6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>the</th>\n","      <th>people</th>\n","      <th>said</th>\n","      <th>of</th>\n","      <th>last</th>\n","      <th>two</th>\n","      <th>financial</th>\n","      <th>years</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>the</th>\n","      <td>0.00023</td>\n","      <td>0.00127</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00353</td>\n","      <td>0.00437</td>\n","      <td>0.00738</td>\n","      <td>0.00046</td>\n","    </tr>\n","    <tr>\n","      <th>people</th>\n","      <td>0.00053</td>\n","      <td>0.00000</td>\n","      <td>0.00044</td>\n","      <td>0.00228</td>\n","      <td>0.00000</td>\n","      <td>0.00018</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>said</th>\n","      <td>0.02941</td>\n","      <td>0.00021</td>\n","      <td>0.00000</td>\n","      <td>0.00125</td>\n","      <td>0.00173</td>\n","      <td>0.00028</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","    <tr>\n","      <th>of</th>\n","      <td>0.17601</td>\n","      <td>0.00278</td>\n","      <td>0.00006</td>\n","      <td>0.00006</td>\n","      <td>0.00096</td>\n","      <td>0.00146</td>\n","      <td>0.00041</td>\n","      <td>0.00026</td>\n","    </tr>\n","    <tr>\n","      <th>last</th>\n","      <td>0.00017</td>\n","      <td>0.00000</td>\n","      <td>0.00017</td>\n","      <td>0.00035</td>\n","      <td>0.00000</td>\n","      <td>0.00148</td>\n","      <td>0.00017</td>\n","      <td>0.00271</td>\n","    </tr>\n","    <tr>\n","      <th>two</th>\n","      <td>0.00000</td>\n","      <td>0.00070</td>\n","      <td>0.00017</td>\n","      <td>0.00321</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00017</td>\n","      <td>0.01086</td>\n","    </tr>\n","    <tr>\n","      <th>financial</th>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00018</td>\n","    </tr>\n","    <tr>\n","      <th>years</th>\n","      <td>0.00291</td>\n","      <td>0.00017</td>\n","      <td>0.00103</td>\n","      <td>0.00806</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               the   people     said       of     last      two  financial  \\\n","the        0.00023  0.00127  0.00000  0.00000  0.00353  0.00437    0.00738   \n","people     0.00053  0.00000  0.00044  0.00228  0.00000  0.00018    0.00000   \n","said       0.02941  0.00021  0.00000  0.00125  0.00173  0.00028    0.00000   \n","of         0.17601  0.00278  0.00006  0.00006  0.00096  0.00146    0.00041   \n","last       0.00017  0.00000  0.00017  0.00035  0.00000  0.00148    0.00017   \n","two        0.00000  0.00070  0.00017  0.00321  0.00000  0.00000    0.00017   \n","financial  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000    0.00000   \n","years      0.00291  0.00017  0.00103  0.00806  0.00000  0.00000    0.00000   \n","\n","             years  \n","the        0.00046  \n","people     0.00000  \n","said       0.00000  \n","of         0.00026  \n","last       0.00271  \n","two        0.01086  \n","financial  0.00018  \n","years      0.00000  "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Table of smoothed bigram probabilities\n","generate_tble(words, tble_smoothed_prob)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsRDWS6yRIPs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljXAVgosRIPs"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"4a68d6c0066e8dcb68188937774911e93882d11496a351f23b3ed5d4f27effc2"}},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}