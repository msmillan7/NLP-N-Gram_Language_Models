{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 3: N-Gram Likehood Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook constitutes the fourth and last pipeline of the N-Gram Language Modeling Analysis Project. It includes the functions to calculate the likehoods, probabilities and perplexities of the different N-Gram Models. It also contains an analysis of the analysis and results for three random sentences picked from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Execute this cell ONLY if you want to work with the testing set\n",
    "#Data for test\n",
    "data_folder = \"output_data/UNK 5-55/\"\n",
    "unigram_pkl = \"unigram_dictionary_test.pkl\"\n",
    "bigram_pkl = \"bigram_dictionary_test.pkl\"\n",
    "trigram_pkl = \"trigram_dictionary_test.pkl\"\n",
    "fourgram_pkl = \"fourgram_dictionary_test.pkl\"\n",
    "test_sentences_unk=\"test_sentences_unk.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Execute this cell ONLY if you want to work with the training set\n",
    "#Data for training\n",
    "unigram_pkl = \"unigram_dictionary_training.pkl\"\n",
    "bigram_pkl = \"bigram_dictionary_training.pkl\"\n",
    "trigram_pkl = \"trigram_dictionary_training.pkl\"\n",
    "fourgram_pkl = \"fourgram_dictionary_training.pkl\"\n",
    "training_sentences_unk=\"training_sentences_unk.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load n-gram dictionaries into the variable *ngram_dicts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(folder, file):\n",
    "    \"\"\"Helper function to load a pkl file\"\"\"\n",
    "    filename = os.path.join(folder, file)\n",
    "    with open(filename, \"rb\") as file: \n",
    "        file_contents = pickle.load(file)\n",
    "    return(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict = load_pkl(data_folder, unigram_pkl)\n",
    "bigram_dict = load_pkl(data_folder, bigram_pkl)\n",
    "trigram_dict = load_pkl(data_folder, trigram_pkl)\n",
    "fourgram_dict = load_pkl(data_folder, fourgram_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the unigram, bigram, trigram, and fourgram dicts together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For MAC\n",
    "#ngram_dict = unigram_dict | bigram_dict | trigram_dict | fourgram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Python 3.7. Same as line above.\n",
    "\n",
    "# Python code to merge dict using update() method\n",
    "def Merge(dict1, dict2):\n",
    "    return(dict1.update(dict2))\n",
    " \n",
    "# This return None\n",
    "Merge(unigram_dict, bigram_dict)\n",
    "Merge(unigram_dict, trigram_dict)\n",
    "Merge(unigram_dict, fourgram_dict)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434612"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_dict = unigram_dict\n",
    "len(unigram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the sentences from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Execute this cell ONLY if you are working with the training set\n",
    "filename = os.path.join(data_folder, training_sentences_unk)\n",
    "with open(filename, \"rb\") as file: \n",
    "    training_sentences_unk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the sentences from the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Execute this cell ONLY if you are working with the test set\n",
    "filename = os.path.join(data_folder, test_sentences_unk)\n",
    "with open(filename, \"rb\") as file: \n",
    "    test_sentences_unk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions: Probability and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_probability(ngram, k = None): \n",
    "    \"\"\" Computes the probability of the given ngram. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    ngram: tple. representing an n-gram of length n>=2\n",
    "    k: float or None. If None is provided, no smoothing is applied. \n",
    "        If a float is provided, add-k smoothing is applied.\n",
    "    \n",
    "    Return\n",
    "    -------------\n",
    "    The probability of the ngram (with or without add-k smoothing)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # print(ngram)\n",
    "    n = len(ngram)\n",
    "    \n",
    "    # Obtain prefix\n",
    "    if n > 2: \n",
    "        prefix = ngram[:(n-1)]\n",
    "    elif n == 2:\n",
    "        prefix = ngram[0]\n",
    "    else: \n",
    "        print(\"ngram must be of length 2 or greater\")\n",
    "        \n",
    "    # print(prefix)\n",
    "    \n",
    "    # No smoothing applied\n",
    "    if k is None: \n",
    "        probability = ngram_dict[ngram]/float(ngram_dict[prefix])\n",
    "        \n",
    "    # Apply add-k smoothing\n",
    "    else:\n",
    "        V = float(len(unigram_dict)) # Vocabulary size\n",
    "        probability = (ngram_dict[ngram] + k)/(ngram_dict[prefix] + (k*V))\n",
    "        \n",
    "    # print(ngram_dict[ngram])\n",
    "    # print(ngram_dict[prefix])\n",
    "    # print(probability)\n",
    "    \n",
    "    return(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probabilities(sentence, n, k=None): \n",
    "    \"\"\" Computes the probability of the given sentence. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    sentence: list. The sentence as a tokenized list\n",
    "    n: int. The degree for the language model. (e.g. n=2 for bigram, n=3 for trigram, etc.)\n",
    "    k: float or None. If None is provided, no smoothing is applied. \n",
    "        If a float is provided, add-k smoothing is applied.\n",
    "    \n",
    "    Return\n",
    "    -------------\n",
    "    A list of probabilities, where index i corresponds to the probability of the ith ngram in the sentence.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain ngrams from the sentence\n",
    "    padded_sentence = list(pad_sequence(sentence,\n",
    "                         pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                         pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                         n=n))\n",
    "    ngram_sentence = list(nltk.ngrams(padded_sentence, n))\n",
    "    \n",
    "    probabilities = [ngram_probability(ngram, k) for ngram in ngram_sentence] \n",
    "    \n",
    "    # print(\"\\nsentence:\", sentence)\n",
    "    # print(\"\\npadded_sentence:\", padded_sentence)\n",
    "    # print(\"\\nngram_sentence:\", ngram_sentence)\n",
    "    \n",
    "    return(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_likelihood(sentence_probabilities): \n",
    "    \"\"\" Computes the likelihood for the sentence. \n",
    "     sentence_probabilites: list. A list of the probabilities for each ngram in a sentence\n",
    "     return: float. The probability of the sentence.\n",
    "    \"\"\"\n",
    "    return np.prod(sentence_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_likelihood(sentence_probabilities):\n",
    "    \"\"\" Computes the log-likelihood for the sentence.\n",
    "     sentence_probabilites: list. A list of the probabilities for each ngram in a sentence\n",
    "     return: float. The log-likelihood of the sentence.\n",
    "    \"\"\"\n",
    "    log_probabilities = [math.log(val) for val in sentence_probabilities]\n",
    "    return(np.sum(log_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_perplexity(sentence_probabilities):\n",
    "    \"\"\" Computes the perplexity for the sentence.\n",
    "     sentence_probabilites: list. A list of the probabilities for each ngram in a sentence\n",
    "     return: float. The log-likelihood of the sentence.\n",
    "    \"\"\"\n",
    "    probability = np.prod(sentence_probabilities)\n",
    "    N = len(sentence_probabilities) # todo check that \n",
    "    return probability ** (-1.0/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_statistics(corpus, n, k):\n",
    "    \"\"\" Compute the average log likelihood and perplexity for a corpus. Uses a n-gram model with or without add-k smoothing\"\"\"\n",
    "    \n",
    "    log_likelihoods = []\n",
    "    perplexities = []\n",
    "\n",
    "    for sentence in corpus: \n",
    "        \n",
    "        # Get the list of probabilities for each ngram in the sentence\n",
    "        ngram_probabilities = sentence_probabilities(sentence, n, k)\n",
    "\n",
    "        # Get the log likelihood of the sentence\n",
    "        log_likelihood = sentence_log_likelihood(ngram_probabilities)\n",
    "        \n",
    "        # Get the perplexity of the sentence\n",
    "        perplexity = sentence_perplexity(ngram_probabilities)\n",
    "\n",
    "        # Store the statistics for the given sentence\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "        perplexities.append(perplexity)\n",
    "\n",
    "    avg_log_likelihood = np.mean(log_likelihoods)\n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    \n",
    "    return({\"avg_log_likelihood\":avg_log_likelihood, \"avg_perplexity\":avg_perplexity})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No smoothing\n",
      "bigram: {'avg_log_likelihood': -81.50553892566927, 'avg_perplexity': 48.07546904387368}\n",
      "trigram: {'avg_log_likelihood': -32.10712816176196, 'avg_perplexity': 4.648789629178389}\n",
      "fourgram: {'avg_log_likelihood': -13.264335791603559, 'avg_perplexity': 2.022652103987471}\n",
      "\n",
      "add-1 smoothing\n",
      "bigram: {'avg_log_likelihood': -234.21628779103438, 'avg_perplexity': 59551.838696568375}\n",
      "trigram: {'avg_log_likelihood': -265.2576563900911, 'avg_perplexity': 131353.90587832712}\n",
      "fourgram: {'avg_log_likelihood': -280.5707084126525, 'avg_perplexity': 150665.81848706104}\n",
      "\n",
      "add-0.25 smoothing\n",
      "bigram: {'avg_log_likelihood': -209.95632327272435, 'avg_perplexity': 19639.11448145478}\n",
      "trigram: {'avg_log_likelihood': -242.58761757715416, 'avg_perplexity': 48261.50347826865}\n",
      "fourgram: {'avg_log_likelihood': -257.7304272790726, 'avg_perplexity': 57109.21649882589}\n"
     ]
    }
   ],
   "source": [
    "#NOTE: Assign to corpus the appropriate value and comment the other line\n",
    "corpus = test_sentences_unk #NOTE: Use this line ONLY if you are working with the test set. Comment otherwise.\n",
    "#corpus = training_sentences_unk #NOTE: Use this line ONLY if you are working with the training set. Comment otherwise.\n",
    "\n",
    "n_values = [2, 3, 4]\n",
    "k_values = [None, 1, 0.25]\n",
    "\n",
    "# No smoothing\n",
    "print(\"\\nNo smoothing\")\n",
    "print(f\"bigram: {corpus_statistics(corpus, 2, None)}\")\n",
    "print(f\"trigram: {corpus_statistics(corpus, 3, None)}\")\n",
    "print(f\"fourgram: {corpus_statistics(corpus, 4, None)}\")\n",
    "\n",
    "# add-1 smoothing\n",
    "print(\"\\nadd-1 smoothing\")\n",
    "print(f\"bigram: {corpus_statistics(corpus, 2, 1)}\")\n",
    "print(f\"trigram: {corpus_statistics(corpus, 3, 1)}\")\n",
    "print(f\"fourgram: {corpus_statistics(corpus, 4, 1)}\")\n",
    "\n",
    "# add-0.25 smoothing\n",
    "print(\"\\nadd-0.25 smoothing\")\n",
    "print(f\"bigram: {corpus_statistics(corpus, 2, 0.25)}\")\n",
    "print(f\"trigram: {corpus_statistics(corpus, 3, 0.25)}\")\n",
    "print(f\"fourgram: {corpus_statistics(corpus, 4, 0.25)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis for Specific Sentences of the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some sample sentences from the training set (three is enough) and show the likelihood calculation for each n-gram (n=2,3,4) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38008"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get sentences in training set\n",
    "training_sentences_unk = \"training_sentences_unk.pkl\"\n",
    "\n",
    "filename = os.path.join(data_folder, training_sentences_unk)\n",
    "with open(filename, \"rb\") as file: \n",
    "    training_sentences_unk = pickle.load(file)\n",
    "\n",
    "len(training_sentences_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:  ['these', 'people', 'are', 'very', 'vulnerable', 'and', 'often', 'easily', '<UNK>']\n",
      "Sentence 2:  ['however', 'the', 'ifc', 'has', 'made', 'no', 'commitment', 'on', 'the', 'next', 'five', 'dams']\n",
      "Sentence 3:  ['so', 'you', 'get', 'the', 'argument', 'why', 'not', 'do', 'biology', '?']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "#Get three random sentences from the trainig set\n",
    "index1, index2, index3 = random.sample(range(0, len(training_sentences_unk)), 3)\n",
    "sentence1 = training_sentences_unk[index1]\n",
    "print(\"Sentence 1: \", sentence1)\n",
    "sentence2 = training_sentences_unk[index2]\n",
    "print(\"Sentence 2: \", sentence2)\n",
    "sentence3 = training_sentences_unk[index3]\n",
    "print(\"Sentence 3: \", sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['these', 'people', 'are', 'very', 'vulnerable', 'and', 'often', 'easily', '\\<UNK\\>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004630604083350874, 0.02932551319648094, 0.04632867132867133, 0.005097312326227989, 0.0045662100456621, 0.10714285714285714, 0.0008663634394628547, 0.004484304932735426, 0.140625, 0.0774194765741457]\n"
     ]
    }
   ],
   "source": [
    "# Get the list of probabilities for each bigram in the sentence\n",
    "sentence1_bigrams_probab = sentence_probabilities(sentence1, 2)\n",
    "print(sentence1_bigrams_probab)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-41.85661864843611\n"
     ]
    }
   ],
   "source": [
    "# Get the log likelihood of the sentence\n",
    "print(sentence_log_likelihood(sentence1_bigrams_probab)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004630604083350874, 0.03409090909090909, 0.25, 0.03773584905660377, 0.045454545454545456, 0.5, 0.3333333333333333, 0.0625, 1.0, 0.2222222222222222, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Get the list of probabilities for each trigram in the sentence\n",
    "sentence1_trigrams_probab = sentence_probabilities(sentence1, 3)\n",
    "print(sentence1_trigrams_probab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.576699609353195\n"
     ]
    }
   ],
   "source": [
    "# Get the log likelihood of the sentence\n",
    "print(sentence_log_likelihood(sentence1_trigrams_probab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Four-Gram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004630604083350874, 0.03409090909090909, 0.5, 0.2, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Get the list of probabilities for each fourgram in the sentence\n",
    "sentence1_fourgrams_probab = sentence_probabilities(sentence1, 4)\n",
    "print(sentence1_fourgrams_probab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.749524747192693\n"
     ]
    }
   ],
   "source": [
    "# Get the log likelihood of the sentence\n",
    "print(sentence_log_likelihood(sentence1_fourgrams_probab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence 1, Sentence 2 and Sentence 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same process (likehood calculation) for the three sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ['these', 'people', 'are', 'very', 'vulnerable', 'and', 'often', 'easily', '<UNK>']\n",
      "---- Bigram ----\n",
      ">>> Bigram probabilities:  [0.004630604083350874, 0.02932551319648094, 0.04632867132867133, 0.005097312326227989, 0.0045662100456621, 0.10714285714285714, 0.0008663634394628547, 0.004484304932735426, 0.140625, 0.0774194765741457]\n",
      ">>> Sentence Log Likehood:  -41.85661864843611\n",
      "---- Trigram ----\n",
      ">>> Trigram probabilities:  [0.004630604083350874, 0.03409090909090909, 0.25, 0.03773584905660377, 0.045454545454545456, 0.5, 0.3333333333333333, 0.0625, 1.0, 0.2222222222222222, 1.0]\n",
      ">>> Sentence Log Likehood:  -22.576699609353195\n",
      "---- Four-gram ----\n",
      ">>> Four-gram probabilities:  [0.004630604083350874, 0.03409090909090909, 0.5, 0.2, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      ">>> Sentence Log Likehood:  -11.749524747192693\n",
      "-----\n",
      "\n",
      "Sentence:  ['however', 'the', 'ifc', 'has', 'made', 'no', 'commitment', 'on', 'the', 'next', 'five', 'dams']\n",
      "---- Bigram ----\n",
      ">>> Bigram probabilities:  [0.008550831403914966, 0.14467408585055644, 0.00029895962052058837, 0.1, 0.01092001092001092, 0.030303030303030304, 0.003590664272890485, 0.01282051282051282, 0.2971000935453695, 0.00460397815601706, 0.024444444444444446, 0.0031446540880503146, 0.06382978723404255]\n",
      ">>> Sentence Log Likehood:  -53.9320381221627\n",
      "---- Trigram ----\n",
      ">>> Trigram probabilities:  [0.008550831403914966, 0.21846153846153846, 0.01098901098901099, 0.13333333333333333, 0.5, 0.075, 0.17647058823529413, 0.25, 1.0, 0.001889168765743073, 0.047619047619047616, 0.09090909090909091, 1.0, 1.0]\n",
      ">>> Sentence Log Likehood:  -30.926980419059092\n",
      "---- Four-gram ----\n",
      ">>> Four-gram probabilities:  [0.008550831403914966, 0.21846153846153846, 0.014084507042253521, 1.0, 0.5, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 0.09090909090909091, 1.0, 1.0, 1.0]\n",
      ">>> Sentence Log Likehood:  -16.9324312622294\n",
      "-----\n",
      "\n",
      "Sentence:  ['so', 'you', 'get', 'the', 'argument', 'why', 'not', 'do', 'biology', '?']\n",
      "---- Bigram ----\n",
      ">>> Bigram probabilities:  [0.0041307093243527675, 0.0044444444444444444, 0.013062409288824383, 0.1099476439790576, 0.00017937577231235302, 0.06451612903225806, 0.046632124352331605, 0.0016474464579901153, 0.0015772870662460567, 0.04, 0.9978354978354979]\n",
      ">>> Sentence Log Likehood:  -47.96512794355295\n",
      "---- Trigram ----\n",
      ">>> Trigram probabilities:  [0.0041307093243527675, 0.006369426751592357, 0.25, 0.2222222222222222, 0.023809523809523808, 0.1111111111111111, 1.0, 0.2222222222222222, 0.2, 1.0, 1.0, 1.0]\n",
      ">>> Sentence Log Likehood:  -22.484333205592883\n",
      "---- Four-gram ----\n",
      ">>> Four-gram probabilities:  [0.0041307093243527675, 0.006369426751592357, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]\n",
      ">>> Sentence Log Likehood:  -11.931846303986648\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [sentence1, sentence2, sentence3]\n",
    "\n",
    "ngram_id_dict = {2:\"Bigram\", 3:\"Trigram\", 4:\"Four-gram\"}\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence: \", sentence)\n",
    "    for k in range(2,5):\n",
    "        print(\"----\", ngram_id_dict[k], \"----\")\n",
    "        probs = sentence_probabilities(sentence, k) #Add 3rd param for smoothing\n",
    "        print(\">>>\", ngram_id_dict[k] , \"probabilities: \", probs)\n",
    "        print(\">>> Sentence Log Likehood: \", sentence_log_likelihood(probs))\n",
    "    print(\"-----\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a68d6c0066e8dcb68188937774911e93882d11496a351f23b3ed5d4f27effc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
